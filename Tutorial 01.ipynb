{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfe1042-0595-412d-b5d8-2f92d94013f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Student Name: Mohammed Alnajjar\n",
    "## ID: 4102947"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b5ce77a-c5ed-450d-9683-d5918ca2f51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddbb77b8-6f8b-41f5-a250-547a61ce9a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/mo-\n",
      "[nltk_data]     alnajjar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/mo-\n",
      "[nltk_data]     alnajjar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "876e626b-4d2c-44e4-858b-ade730b78d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_document(document):\n",
    "    # Tokenize the document\n",
    "    tokens = word_tokenize(document)\n",
    "    \n",
    "    # Remove punctuation and convert to lowercase\n",
    "    tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20e5b058-e834-4bfe-b34e-3e9051c5fc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bow(document, vocabulary):\n",
    "    bow_vector = [0] * len(vocabulary)\n",
    "    for token in document:\n",
    "        if token in vocabulary:\n",
    "            bow_vector[vocabulary.index(token)] += 1\n",
    "    return bow_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb7fda29-40ea-4193-85f4-b46ac0ce6e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(documents):\n",
    "    vocabulary = set()\n",
    "    for document in documents:\n",
    "        vocabulary.update(document)\n",
    "    return sorted(list(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "842d82fc-85e0-4844-80c1-3306db78f5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def cosine_similarity(vector1, vector2):\n",
    "    dot_product = sum(x * y for x, y in zip(vector1, vector2))\n",
    "    norm1 = math.sqrt(sum(x ** 2 for x in vector1))\n",
    "    norm2 = math.sqrt(sum(x ** 2 for x in vector2))\n",
    "    similarity = dot_product / (norm1 * norm2)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d4e8e08-fffc-49a2-897d-a78d38232d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.0, 0.6123724356957946, 0.0]\n",
      "[0.0, 1.0, 0.4082482904638631, 0.4472135954999579]\n",
      "[0.6123724356957946, 0.4082482904638631, 1.0000000000000002, 0.3651483716701107]\n",
      "[0.0, 0.4472135954999579, 0.3651483716701107, 0.9999999999999998]\n"
     ]
    }
   ],
   "source": [
    "# Sample documents\n",
    "documents = [\n",
    "\"I love natural language processing\",\n",
    "\"Information retrieval is an interesting topic\",\n",
    "\"Natural language processing and information retrieval are related\",\n",
    "\"I enjoy working on information retrieval projects\"\n",
    "]\n",
    "\n",
    "# Preprocess the documents\n",
    "preprocessed_documents = [preprocess_document(doc) for doc in documents]\n",
    "\n",
    "# Create the vocabulary\n",
    "vocabulary = create_vocabulary(preprocessed_documents)\n",
    "\n",
    "# Create the BoW representation for each document\n",
    "bow_vectors = [create_bow(doc, vocabulary) for doc in preprocessed_documents]\n",
    "\n",
    "# Calculate document similarities\n",
    "similarity_matrix = []\n",
    "for i in range(len(bow_vectors)):\n",
    "    row = []\n",
    "    for j in range(len(bow_vectors)):\n",
    "        similarity = cosine_similarity(bow_vectors[i], bow_vectors[j])\n",
    "        row.append(similarity)\n",
    "    similarity_matrix.append(row)\n",
    "    \n",
    "# Print the similarity matrix\n",
    "for row in similarity_matrix:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7618b5d4-e8e5-4f3c-ab56-561d94de8409",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
